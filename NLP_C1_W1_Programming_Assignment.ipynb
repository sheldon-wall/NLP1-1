{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 1: Logistic Regression\n",
    "Welcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\n",
    "\n",
    "* Learn how to extract features for logistic regression given some text\n",
    "* Implement logistic regression from scratch\n",
    "* Apply logistic regression on a natural language processing task\n",
    "* Test using your logistic regression\n",
    "* Perform error analysis\n",
    "\n",
    "We will be using a data set of tweets. Hopefully you will get more than 99% accuracy.\n",
    "Run the cell below to load in the packages."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from os import getcwd\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "from utils import process_tweet, build_freqs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n[nltk_data]     C:\\Users\\sheld\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\sheld\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "train_y.shape = (8000, 1)\ntest_y.shape = (2000, 1)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "\n",
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "type(freqs) = <class 'dict'>\nlen(freqs) = 11340\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "This is an example of a positive tweet: \n #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n\nThis is an example of the processed version of the tweet: \n ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Test the Process Tweet function\n",
    "\n",
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Logistic regression \n",
    "\n",
    "\n",
    "### Part 1.1: Sigmoid\n",
    "You will learn to use logistic regression for text classification. \n",
    "* The sigmoid function is defined as: \n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='../tmp2/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # calculate the sigmoid of z\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return h "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "SUCCESS!\nCORRECT!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "## Test the sigmoid function \n",
    "\n",
    "# Testing your function \n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')\n",
    "\n",
    "if (sigmoid(4.92) == 0.9927537604041685):\n",
    "    print('CORRECT!')\n",
    "else:\n",
    "    print('Oops again!')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic regression: regression and a sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Note that the $\\theta$ values are \"weights\". If you took the Deep Learning Specialization, we referred to the weights with the `w` vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "We will refer to 'z' as the 'logits'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1.2 Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "9.210340371976294"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "## Verify that when the model predicts close to 1, but the actual label is 0, the loss is large pos\n",
    "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "9.210340371976182"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "## Likewise verify that when the model predicts close to 0 but label is 1, the first term becomes large\n",
    "-1 * np.log(0.0001) # loss is about 9.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Update the weights\n",
    "\n",
    "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The cost after training is 0.67094970.\nThe resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = len(x)\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = -(1/m) * (np.dot(y.T, np.log(h)) + np.dot(1-y.T, np.log(1-h)))\n",
    "        \n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha/m) * (np.dot(x.T, (h - y)))\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    J = float(J)\n",
    "    return J, theta\n",
    "\n",
    "# Check the function\n",
    "# Construct a synthetic test case using numpy PRNG functions\n",
    "np.random.seed(1)\n",
    "# X input is 10 x 3 with ones for the bias terms\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y Labels are 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Extracting the features\n",
    "\n",
    "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet. \n",
    "* Then train your logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set. \n",
    "\n",
    "### Instructions: Implement the extract_features function. \n",
    "* This function takes in a single tweet.\n",
    "* Process the tweet using the imported `process_tweet()` function and save the list of tweet words.\n",
    "* Loop through each word in the list of processed words\n",
    "    * For each word, check the `freqs` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
    "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract Features\n",
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        if (word, 1) in freqs:\n",
    "            x[0,1] += freqs[(word, 1)]\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        if (word, 0) in freqs:\n",
    "            x[0,2] += freqs[(word, 0)]\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n",
    "\n",
    "# Check your function\n",
    "\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1.00e+00 3.02e+03 6.10e+01]]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[1. 0. 0.]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1. 0. 0.]]\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Training Your Model\n",
    "\n",
    "To train the model:\n",
    "* Stack the features for all training examples into a matrix `X`. \n",
    "* Call `gradientDescent`, which you've implemented above.\n",
    "\n",
    "This section is given to you.  Please read it for understanding and run the cell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The cost after training is 0.24216477.\nThe resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected Output**: \n",
    "\n",
    "```\n",
    "The cost after training is 0.24216529.\n",
    "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n",
    "```\n",
    "\n",
    "# Part 4: Test your logistic regression\n",
    "\n",
    "It is time for you to test your logistic regression function on some new input that your model has not seen before. \n",
    "\n",
    "#### Instructions: Write `predict_tweet`\n",
    "Predict whether a tweet is positive or negative.\n",
    "\n",
    "* Given a tweet, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "I am happy -> 0.518580\nI am bad -> 0.494339\nthis movie should have been great. -> 0.515331\ngreat -> 0.515464\ngreat great -> 0.530898\ngreat great great -> 0.546273\ngreat great great great -> 0.561561\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected Output**: \n",
    "```\n",
    "I am happy -> 0.518580\n",
    "I am bad -> 0.494339\n",
    "this movie should have been great. -> 0.515331\n",
    "great -> 0.515464\n",
    "great great -> 0.530898\n",
    "great great great -> 0.546273\n",
    "great great great great -> 0.561561\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.81636482]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check performance using the test set\n",
    "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
    "\n",
    "#### Instructions: Implement `test_logistic_regression` \n",
    "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model. \n",
    "* Use your `predict_tweet()` function to make predictions on each tweet in the test set.\n",
    "* If the prediction is > 0.5, set the model's classification `y_hat` to 1, otherwise set the model's classification `y_hat` to 0.\n",
    "* A prediction is accurate when `y_hat` equals `test_y`.  Sum up all the instances when they are equal and divide by `m`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = sum(np.array(y_hat) == np.squeeze(test_y)) / len(test_x)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 5: Error Analysis\n",
    "\n",
    "In this part you will see some tweets that your model misclassified. Why do you think the misclassifications happened? Specifically what kind of tweets does your model misclassify?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Label Predicted Tweet\nTHE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\nTHE PROCESSED TWEET IS: ['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n1\t0.49996897\tb'truli later move know queen bee upward bound movingonup'\nTHE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\nTHE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n1\t0.48650628\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\nTHE TWEET IS: I'm playing Brain Dots : ) #BrainDots\nhttp://t.co/UGQzOx0huu\nTHE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n1\t0.48370676\tb\"i'm play brain dot braindot\"\nTHE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\nTHE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\nTHE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n1\t0.48370676\tb\"i'm play brain dot braindot\"\nTHE TWEET IS: off to the park to get some sunlight : )\nTHE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n1\t0.49578773\tb'park get sunlight'\nTHE TWEET IS:",
      " @msarosh Uff Itna Miss karhy thy ap :p\nTHE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n1\t0.48199817\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\nTHE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n0\t0.50020361\tb'u prob fun david'\nTHE TWEET IS:",
      " pats jay : (\nTHE PROCESSED TWEET IS: ['pat', 'jay']\n0\t0.50039294\tb'pat jay'\nTHE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\nTHE PROCESSED TWEET IS: ['belov', 'grandmoth']\n0\t0.50000002\tb'belov grandmoth'\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 6: Predict with your own tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n[[0.48139091]]\nNegative sentiment\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}